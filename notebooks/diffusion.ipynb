{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "from torchvision import utils as vutils\n",
    "from torchvision.transforms.v2.functional import to_pil_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.load(\"../data/train_data.pth\", weights_only=False)\n",
    "test_data = torch.load(\"../data/test_data.pth\", weights_only=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF:\n",
    "    def __init__(self, model, ln=True) -> None:\n",
    "        self.model = model\n",
    "        self.ln = ln\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        b = x.size(0)\n",
    "        if self.ln:\n",
    "            nt = torch.randn((b, 1), device=x.device)\n",
    "            t = torch.sigmoid(nt)\n",
    "        else:\n",
    "            t = torch.rand((b, 1), device=x.device)\n",
    "\n",
    "        texp = t.view([b, *([1] * len(x.shape[1:]))])\n",
    "        z1 = torch.randn_like(x)\n",
    "        zt = (1 - texp) * x + texp * z1\n",
    "        vtheta = self.model(zt, t, cond)\n",
    "        batchwise_mse = ((z1 - x - vtheta) ** 2).mean(dim=list(range(1, len(x.shape))))\n",
    "        tlist = batchwise_mse.detach().cpu().reshape(-1).tolist()\n",
    "        ttloss = list(zip(t, tlist))\n",
    "        return batchwise_mse.mean(), ttloss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, z, cond, null_cond=None, sample_steps=50, cfg=2.0):\n",
    "        b = z.size(0)\n",
    "        dt = 1.0 / sample_steps\n",
    "        dt = torch.tensor([dt] * b).to(z.device).view([b, *([1] * len(z.shape[1:]))])\n",
    "        images = [z]\n",
    "        for i in range(sample_steps, 0, -1):\n",
    "            t = i / sample_steps\n",
    "            t = torch.tensor([t] * b, device=z.device)\n",
    "\n",
    "            vc = self.model(z, t, cond)\n",
    "            if null_cond is not None:\n",
    "                vu = self.model(z, t, null_cond)\n",
    "                vc = vu + cfg * (vc - vu)\n",
    "\n",
    "            z = z - dt * vc\n",
    "            images.append(z)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from digitdreamer import DiT\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = DiT().to(device)\n",
    "rf = RF(model)\n",
    "summary(model, input_size=(128, 8, 2, 2), depth=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y += 1\n",
    "\n",
    "        drop = torch.rand(x.shape[0])\n",
    "        y[drop < 0.1] = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss, _ = rf.forward(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix_str(f\"loss: {loss.item():.4f}, val_loss: {val_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y += 1\n",
    "            loss, _ = rf.forward(x, y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from digitdreamer import Autoencoder\n",
    "\n",
    "autoencoder = Autoencoder()\n",
    "autoencoder.encoder.load_state_dict(\n",
    "    torch.load(\"../models/encoder.pth\", weights_only=True),\n",
    ")\n",
    "autoencoder.decoder.load_state_dict(\n",
    "    torch.load(\"../models/decoder.pth\", weights_only=True),\n",
    ")\n",
    "\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval()\n",
    "\n",
    "# generate samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(10, 8, 2, 2, device=device)\n",
    "    cond = torch.arange(10, device=device, dtype=torch.long).repeat(1) + 1\n",
    "    uncond = torch.zeros(10, device=device, dtype=torch.long)\n",
    "\n",
    "    samples = rf.sample(noise, cond, uncond, sample_steps=12, cfg=2.25)\n",
    "    samples = torch.cat(samples, dim=0)\n",
    "\n",
    "    imgs = autoencoder.decoder(samples).cpu()\n",
    "    imgs = imgs.view(-1, 10, 1, 32, 32)\n",
    "\n",
    "    pil_imgs = [to_pil_image(vutils.make_grid(img, nrow=10)) for img in imgs]\n",
    "    pil_imgs = pil_imgs + pil_imgs[::-1]\n",
    "\n",
    "    # save as gif\n",
    "    pil_imgs[0].save(\n",
    "        \"../assets/samples.gif\",\n",
    "        save_all=True,\n",
    "        append_images=pil_imgs[1:],\n",
    "        duration=100,\n",
    "        loop=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../models/diffusion.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"../models/encoder.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
