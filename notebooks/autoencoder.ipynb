{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from torchvision import utils as vutils\n",
    "from tqdm import tqdm\n",
    "\n",
    "from digitdreamer import mnistdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = mnistdm.MNISTDataModule(\"../data\") # use the other one for fine-tuning\n",
    "# dm = mnistdm.MNISTDataModule(\n",
    "#     data_dir=\"../data\",\n",
    "#     degrees=(0, 0),\n",
    "#     translate=(0, 0),\n",
    "#     scale=(1, 1),\n",
    "#     shear=(-0, 0),\n",
    "# )\n",
    "dm.setup()\n",
    "train_loader = dm.train_dataloader()\n",
    "test_loader = dm.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from digitdreamer import Autoencoder, autoencoder, modules\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = Autoencoder().to(device)\n",
    "ae_optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4)\n",
    "summary(model, input_size=(1, 1, 32, 32), device=device, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            *autoencoder.down_block(1, 8),\n",
    "            modules.Block(8, 8),\n",
    "            *autoencoder.down_block(8, 16),\n",
    "            modules.Block(16, 16),\n",
    "            *autoencoder.down_block(16, 32),\n",
    "            modules.Block(32, 32),\n",
    "            nn.Conv2d(32, 1, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "d_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=6e-4)\n",
    "summary(\n",
    "    discriminator,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    input_size=(1, 1, 32, 32),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = 0\n",
    "val_psnr = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        with torch.no_grad():\n",
    "            x_hat = model(x)\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        real = discriminator(x)\n",
    "        fake = discriminator(x_hat)\n",
    "        d_loss = (\n",
    "            F.binary_cross_entropy(fake, torch.zeros_like(fake))\n",
    "            + F.binary_cross_entropy(real, torch.ones_like(real))\n",
    "        ) / 2\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Autoencoder\n",
    "        ae_optimizer.zero_grad()\n",
    "        x_hat = model(x)\n",
    "        d_fake = discriminator(x_hat.detach())\n",
    "        bce_loss = F.binary_cross_entropy(x_hat, x)\n",
    "        adv_loss = F.binary_cross_entropy(d_fake, torch.ones_like(d_fake)) * 1e-2\n",
    "\n",
    "        ae_loss = bce_loss + adv_loss\n",
    "        ae_loss.backward()\n",
    "        ae_optimizer.step()\n",
    "\n",
    "        mse_loss = F.mse_loss(x_hat, x)\n",
    "\n",
    "        pbar.set_postfix_str(\n",
    "            f\"loss: {ae_loss:.4f}, psnr: {10 * torch.log10(1 / mse_loss):.2f}, bce: {bce_loss:.2f}, adv: {adv_loss:.2f}, real: {real.mean():.2f}, fake: {fake.mean():.2f}, val_loss: {val_loss:.4f}, val_psnr: {val_psnr:.2f}\",\n",
    "        )\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_psnr = 0\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            x_hat = model(x)\n",
    "            val_loss += F.mse_loss(x_hat, x)\n",
    "            val_psnr += 10 * torch.log10(1 / F.mse_loss(x_hat, x))\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    val_psnr /= len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x, _ = next(iter(test_loader))\n",
    "    x = x.to(device)\n",
    "    x_hat = model(x)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(\n",
    "        vutils.make_grid(x[:64], nrow=8, normalize=True).cpu().numpy().transpose(1, 2, 0),\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Original\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(\n",
    "        vutils.make_grid(x_hat[:64], nrow=8, normalize=True)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "        .transpose(1, 2, 0),\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Reconstructed\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), \"../models/encoder.pth\")\n",
    "torch.save(model.decoder.state_dict(), \"../models/decoder.pth\")\n",
    "torch.save(discriminator.state_dict(), \"../models/discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder.load_state_dict(torch.load(\"../models/encoder.pth\", weights_only=True))\n",
    "model.decoder.load_state_dict(torch.load(\"../models/decoder.pth\", weights_only=True))\n",
    "discriminator.load_state_dict(\n",
    "    torch.load(\"../models/discriminator.pth\", weights_only=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.encoder.state_dict(), \"../models/ft-encoder.pth\")\n",
    "torch.save(model.decoder.state_dict(), \"../models/ft-decoder.pth\")\n",
    "torch.save(discriminator.state_dict(), \"../models/ft-discriminator.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate latents\n",
    "with torch.no_grad():\n",
    "    latents = []\n",
    "    classes = []\n",
    "    for x, y in tqdm(train_loader):\n",
    "        x = x.to(device)\n",
    "        latents.append(model.encoder(x).cpu())\n",
    "        classes.append(y.cpu())\n",
    "\n",
    "    latents = torch.cat(latents)\n",
    "    classes = torch.cat(classes)\n",
    "    train_data = torch.utils.data.TensorDataset(latents, classes)\n",
    "    torch.save(train_data, \"../data/train_data.pth\")\n",
    "\n",
    "    latents = []\n",
    "    classes = []\n",
    "\n",
    "    for x, y in tqdm(test_loader):\n",
    "        x = x.to(device)\n",
    "        latents.append(model.encoder(x).cpu())\n",
    "        classes.append(y.cpu())\n",
    "\n",
    "    latents = torch.cat(latents)\n",
    "    classes = torch.cat(classes)\n",
    "    test_data = torch.utils.data.TensorDataset(latents, classes)\n",
    "    torch.save(test_data, \"../data/test_data.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
